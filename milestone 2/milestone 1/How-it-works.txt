How This Real-Time IoT Data Pipeline Works (Hybrid Architecture)
================================================================

Overview
--------
This project demonstrates a complete "Hybrid" IoT data pipeline, processing data simultaneously via two paths:
  1) Edge Path (Local): Uses Docker/Kafka/SQLite for offline monitoring.
  2) Cloud Path (Azure): Uses Event Hubs/ADF/ASA/Power BI for global analytics.

The core component is `generator.py`, which acts as the IoT device simulator and feeds both paths instantly.

High‑level data flow
--------------------
                      ┌── [Edge Path] ──► Local Kafka ──► edge_consumer.py ──► SQLite DB ──► Power BI Desktop
                      │
  [generator.py] ─────┼── [Cloud Path] ──► Azure Event Hub ──► Stream Analytics ──► Azure SQL & Power BI Service
                      │
                      └── [Batch Path] ──► sensors.csv (Uploads to Blob on Exit) ──► Data Factory ──► Azure SQL

File by file
------------
1) docker-compose.yml (Local Infrastructure)
   • Purpose: Starts the local messaging backbone.
   • Services:
     - ZooKeeper (Kafka coordination)
     - Kafka broker (Listens on localhost:9092)
   • Role: Allows the "Edge Path" to function entirely offline without an internet connection.

2) generator.py (The Source)
   • Role: Simulates a single IoT device ('dev-1') generating telemetry every 5 seconds.
   • Dual Ingestion:
     - Produces JSON messages to Local Kafka (Topic: 'iotsensors').
     - Produces JSON messages to Azure Event Hubs (Hub: 'iot-stream').
   • Batch Handling:
     - Writes every reading to a local 'sensors.csv' file.
     - On shutdown (Ctrl+C), automatically uploads this CSV to Azure Blob Storage to trigger the Batch Pipeline.
   • Data Schema:
     - device_id, ts (ISO 8601 format), temperature_c, humidity_pct, battery_pct.
     - Simulates data anomalies (e.g., battery draining to 0%, causing null sensor values).

3) edge_consumer.py (The Edge Worker)
   • Role: Connects to Local Kafka and persists data to a local database.
   • Output: Creates and populates 'edge_data.db' (SQLite).
   • Usage: Enables Power BI Desktop to visualize data locally/offline via a Python connector script.

4) consumer.py (Legacy/Debug)
   • Role: A simple console printer that subscribes to Local Kafka.
   • Usage: Used for quick debugging to verify data is flowing locally without checking databases.

Cloud Architecture (Azure)
--------------------------
1) Ingestion (Event Hubs):
   • Receives real-time data from generator.py.
   • Configured with 1 Partition to ensure ordered processing.

2) Speed Layer (Stream Analytics):
   • Job: 'depi-realtime-processor' (1/3 Streaming Units).
   • Logic: Reads from Event Hub and splits data into three outputs:
     - [iot-warehouse]: Filters for alerts (Temp > 30, Battery < 10) -> SQL 'dbo.alerts'.
     - [live-readings]: Dumps raw logs -> SQL 'dbo.live_readings'.
     - [My-workspace]: Enriches data with Status Flags (0/1) -> Power BI Streaming Dataset.

3) Batch Layer (Data Factory):
   • Trigger: Detects when 'sensors.csv' is uploaded to Blob Storage.
   • Pipeline: Copies data from Blob -> Azure SQL 'dbo.sensor_readings'.
   • Logic: Uses Idempotent UPSERT (via Primary Key) to prevent duplicates.

4) Visualization (Power BI Service):
   • Dashboard: 'IoT Live Dashboard'.
   • Features:
     - Real-time Gauges (Battery/Temp) via Custom Streaming Tiles.
     - Status Cards (Normal/Alert) via Text visuals.
     - Historical Line Charts via Report view with 1-second Auto-Refresh.

End‑to‑end execution
--------------------
1) Start Local Services:
     docker compose up -d
2) Start the Generator:
     python generator.py
   (You will see [PRODUCED] logs confirming data is going to both Kafka and Azure).
3) Monitor (Choose your view):
   • Edge View: Run `python edge_consumer.py` to populate local DB.
   • Cloud View: Open Power BI Service to see the dashboard update live.
   • Debug View: Run `python consumer.py` to see raw JSON.
4) Stop & Batch Process:
   • Press Ctrl+C in generator.py.
   • The script automatically uploads 'sensors.csv' to Azure.
   • Check Azure Data Factory Monitor -> The pipeline will trigger automatically.

Quick checklist
---------------
☐ .env file contains both EVENTHUB_CONNECTION_STRING and AZURE_STORAGE_CONNECTION_STRING
☐ 'docker compose up -d' is running
☐ Azure resources (Event Hub, SQL, ASA Job) are deployed and running
☐ Run generator.py → Verifies data flow to Cloud and Local Kafka